{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Training with Faster RCNN\n",
    "\n",
    "We'll import code from the scripts folder so we don't have to clutter this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from albumentations import OneOf\n",
    "\n",
    "sys.path.append(os.path.join('..', 'scripts'))\n",
    "from models import WheatModel\n",
    "from evaluation import calculate_image_precision\n",
    "\n",
    "sns.set()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "ALLOW_AUTOMATIC_SHUTDOWN = True  # allow the script to automatically shutdown your computer after use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Anaconda3\\envs\\wheat-detection\\lib\\site-packages\\torch\\nn\\functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 0.7017498016357422\n",
      "Iteration #100 loss: 0.6411789655685425\n",
      "Iteration #150 loss: 0.8437304496765137\n",
      "Iteration #200 loss: 0.6916768550872803\n",
      "Iteration #250 loss: 0.642479658126831\n",
      "Iteration #300 loss: 0.7822573781013489\n",
      "Iteration #350 loss: 0.8070986270904541\n",
      "Iteration #400 loss: 0.7544382810592651\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_path = os.path.join('..', 'data')\n",
    "\n",
    "model = WheatModel(\n",
    "    base_path=base_path,\n",
    "    num_epochs=11,\n",
    "    train_val_split=1,\n",
    "#     transforms=[A.Flip(0.5), \n",
    "#                 OneOf([\n",
    "#                     A.RandomBrightnessContrast(), \n",
    "#                     A.HueSaturationValue()\n",
    "#                 ], p=0.5),\n",
    "#                 OneOf([\n",
    "#                     A.MotionBlur()\n",
    "#                 ])\n",
    "#                ]\n",
    "    transforms=[]\n",
    ")\n",
    "loss, precisions = model.main()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(loss)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(precisions)\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join('..', 'data')\n",
    "\n",
    "model = WheatModel(\n",
    "    base_path=base_path,\n",
    "    num_epochs=1,\n",
    "    train_val_split=0.8,\n",
    "    weights_file='faster_rcnn_resnet50_fpn_10epochs.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[954, 391,  70,  90],\n",
       "       [660, 220,  95, 102],\n",
       "       [ 64, 209,  76,  57],\n",
       "       [896,  99, 102,  69],\n",
       "       [747, 460,  72,  77],\n",
       "       [885, 163, 103,  69],\n",
       "       [514, 399,  90,  97],\n",
       "       [702, 794,  97,  99],\n",
       "       [721, 624,  98, 108],\n",
       "       [826, 512,  82,  94],\n",
       "       [883, 944,  79,  74],\n",
       "       [247, 594, 123,  92],\n",
       "       [673, 514,  95, 113],\n",
       "       [829, 847, 102, 110],\n",
       "       [ 94, 737,  92, 107],\n",
       "       [588, 568,  75, 107],\n",
       "       [158, 890, 103,  64],\n",
       "       [744, 906,  75,  79],\n",
       "       [826,  33,  72,  74],\n",
       "       [601,  69,  67,  87]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our testing sample\n",
    "sample_id = '1ef16dab1'\n",
    "\n",
    "gt_boxes = model.train_df[model.train_df['image_id'] == sample_id][['x', 'y', 'w', 'h']].values\n",
    "gt_boxes = gt_boxes.astype(np.int)\n",
    "\n",
    "# Ground-truth boxes of our sample\n",
    "gt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('31a6e55f2', 'fd5624913', 'b13f38e6f', '48c72c991')\n",
      "('42e247347', '556c18af4', '14fd84198', 'c682d8c66')\n",
      "('4aeb37788', 'e46378032', '798a261f0', '999d9d27d')\n",
      "('d13e3ffce', '8b558e9e3', '005b0d8bb', 'e99cca2a3')\n",
      "('4c7fc04f6', '372d12c55', '43e3ccd92', '4e5c1078d')\n",
      "('69fc3d3ff', 'c6b4933f7', '1bda74f14', '92c92d40d')\n",
      "('895420d9f', 'b815af10b', '5e0a7fb4d', '220488d01')\n",
      "('41c0123cc', '8d6bb70e5', 'c86621747', 'c73f0a714')\n",
      "('33d37a437', 'c3c03e0b7', '7b72ea0fb', '480c01244')\n",
      "('afb7671e5', '029c79463', 'f251e40f2', '5738fcf14')\n",
      "('4a3f67d2e', '49dcb7dd3', '4d90cdad1', '29e44e305')\n",
      "('4c02b2f7f', '83a5d8ac8', '2ccec6dcc', '863e0253e')\n",
      "('d7d200c0b', 'f5093f57d', 'ca4cb597b', 'b6ab77fd7')\n",
      "('a22cdd5eb', '91c9d9c38', '43a765c3e', 'b53afdf5c')\n",
      "('3e1e66c76', '71fd03d51', 'cc9092474', '0435654cd')\n",
      "('01f37f1d3', '43650fd9c', '695ae4a12', 'b7c97a57b')\n",
      "('826d92f06', '531acb360', '02b6f199c')\n",
      "0.7661366651953706\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "model.model.eval()\n",
    "for images, targets, image_ids in model.valid_data_loader:\n",
    "    images = list(image.to(model.device) for image in images)\n",
    "    outputs = model.model.forward(images)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[i]\n",
    "\n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "        gt_boxes = model.val_df[model.val_df['image_id'] == image_ids[i]][['x', 'y', 'w', 'h']].values\n",
    "        image_precision = calculate_image_precision(gt_boxes, boxes)\n",
    "        precisions.append(image_precision)\n",
    "\n",
    "print(np.mean(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Anaconda3\\envs\\wheat-detection\\lib\\site-packages\\torch\\nn\\functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    }
   ],
   "source": [
    "model.model.eval()\n",
    "detection_threshold = 0.5\n",
    "results = []\n",
    "\n",
    "for images, targets, image_ids in model.valid_data_loader:\n",
    "\n",
    "    images = list(image.to(model.device) for image in images)\n",
    "    outputs = model.model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "\n",
    "        \n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################################################################################################\n",
    "# #################### WARNING THIS WILL SHUT DOWN THE COMPUTER IF IT IS UNCOMMENTED ################################\n",
    "# ###################################################################################################################\n",
    "# if ALLOW_AUTOMATIC_SHUTDOWN:\n",
    "#     os.system('shutdown -s -t 0')\n",
    "# ###################################################################################################################\n",
    "# #################### WARNING THIS WILL SHUT DOWN THE COMPUTER IF IT IS UNCOMMENTED ################################\n",
    "# ###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
