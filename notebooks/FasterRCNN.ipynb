{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Training with Faster RCNN\n",
    "\n",
    "We'll import code from the scripts folder so we don't have to clutter this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from albumentations import OneOf\n",
    "\n",
    "sys.path.append(os.path.join('..', 'scripts'))\n",
    "from models import WheatModel\n",
    "from evaluation import calculate_image_precision\n",
    "\n",
    "sns.set()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "ALLOW_AUTOMATIC_SHUTDOWN = True  # allow the script to automatically shutdown your computer after use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Anaconda3\\envs\\wheat-detection\\lib\\site-packages\\torch\\nn\\functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 0.7665133476257324\n",
      "Iteration #100 loss: 0.7659050226211548\n",
      "Iteration #150 loss: 0.6503384709358215\n",
      "Iteration #200 loss: 0.6491904258728027\n",
      "Iteration #250 loss: 0.6501611471176147\n",
      "Iteration #300 loss: 0.6963229775428772\n",
      "Iteration #350 loss: 0.9552693963050842\n",
      "Iteration #400 loss: 0.6252937316894531\n",
      "Iteration #450 loss: 0.6493210792541504\n",
      "Iteration #500 loss: 0.6280516386032104\n",
      "Iteration #550 loss: 0.5894546508789062\n",
      "Iteration #600 loss: 0.6604169607162476\n",
      "Iteration #650 loss: 0.8331085443496704\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\wheat-detection\\scripts\\models.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m                 \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m                 \u001b[0mprecisions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mvalidation_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\wheat-detection\\scripts\\models.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_path = os.path.join('..', 'data')\n",
    "\n",
    "model = WheatModel(\n",
    "    base_path=base_path,\n",
    "    num_epochs=20,\n",
    "    train_val_split=0.8,\n",
    "#     transforms=[A.Flip(0.5), \n",
    "#                 OneOf([\n",
    "#                     A.RandomBrightnessContrast(), \n",
    "#                     A.HueSaturationValue()\n",
    "#                 ], p=0.5),\n",
    "#                 A.MotionBlur()\n",
    "#                ]\n",
    "    transforms=None\n",
    ")\n",
    "loss, precisions, validation_losses = model.main()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(loss)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(validation_losses)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(precisions)\n",
    "# model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join('..', 'data')\n",
    "\n",
    "model = WheatModel(\n",
    "    base_path=base_path,\n",
    "    num_epochs=1,\n",
    "    train_val_split=0.8,\n",
    "    weights_file='faster_rcnn_resnet50_fpn_10epochs.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[954, 391,  70,  90],\n",
       "       [660, 220,  95, 102],\n",
       "       [ 64, 209,  76,  57],\n",
       "       [896,  99, 102,  69],\n",
       "       [747, 460,  72,  77],\n",
       "       [885, 163, 103,  69],\n",
       "       [514, 399,  90,  97],\n",
       "       [702, 794,  97,  99],\n",
       "       [721, 624,  98, 108],\n",
       "       [826, 512,  82,  94],\n",
       "       [883, 944,  79,  74],\n",
       "       [247, 594, 123,  92],\n",
       "       [673, 514,  95, 113],\n",
       "       [829, 847, 102, 110],\n",
       "       [ 94, 737,  92, 107],\n",
       "       [588, 568,  75, 107],\n",
       "       [158, 890, 103,  64],\n",
       "       [744, 906,  75,  79],\n",
       "       [826,  33,  72,  74],\n",
       "       [601,  69,  67,  87]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our testing sample\n",
    "sample_id = '1ef16dab1'\n",
    "\n",
    "gt_boxes = model.train_df[model.train_df['image_id'] == sample_id][['x', 'y', 'w', 'h']].values\n",
    "gt_boxes = gt_boxes.astype(np.int)\n",
    "\n",
    "# Ground-truth boxes of our sample\n",
    "gt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('31a6e55f2', 'fd5624913', 'b13f38e6f', '48c72c991')\n",
      "('42e247347', '556c18af4', '14fd84198', 'c682d8c66')\n",
      "('4aeb37788', 'e46378032', '798a261f0', '999d9d27d')\n",
      "('d13e3ffce', '8b558e9e3', '005b0d8bb', 'e99cca2a3')\n",
      "('4c7fc04f6', '372d12c55', '43e3ccd92', '4e5c1078d')\n",
      "('69fc3d3ff', 'c6b4933f7', '1bda74f14', '92c92d40d')\n",
      "('895420d9f', 'b815af10b', '5e0a7fb4d', '220488d01')\n",
      "('41c0123cc', '8d6bb70e5', 'c86621747', 'c73f0a714')\n",
      "('33d37a437', 'c3c03e0b7', '7b72ea0fb', '480c01244')\n",
      "('afb7671e5', '029c79463', 'f251e40f2', '5738fcf14')\n",
      "('4a3f67d2e', '49dcb7dd3', '4d90cdad1', '29e44e305')\n",
      "('4c02b2f7f', '83a5d8ac8', '2ccec6dcc', '863e0253e')\n",
      "('d7d200c0b', 'f5093f57d', 'ca4cb597b', 'b6ab77fd7')\n",
      "('a22cdd5eb', '91c9d9c38', '43a765c3e', 'b53afdf5c')\n",
      "('3e1e66c76', '71fd03d51', 'cc9092474', '0435654cd')\n",
      "('01f37f1d3', '43650fd9c', '695ae4a12', 'b7c97a57b')\n",
      "('826d92f06', '531acb360', '02b6f199c')\n",
      "0.7661366651953706\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "model.model.eval()\n",
    "for images, targets, image_ids in model.valid_data_loader:\n",
    "    images = list(image.to(model.device) for image in images)\n",
    "    outputs = model.model.forward(images)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[i]\n",
    "\n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "        gt_boxes = model.val_df[model.val_df['image_id'] == image_ids[i]][['x', 'y', 'w', 'h']].values\n",
    "        image_precision = calculate_image_precision(gt_boxes, boxes)\n",
    "        precisions.append(image_precision)\n",
    "\n",
    "print(np.mean(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Anaconda3\\envs\\wheat-detection\\lib\\site-packages\\torch\\nn\\functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    }
   ],
   "source": [
    "model.model.eval()\n",
    "detection_threshold = 0.5\n",
    "results = []\n",
    "\n",
    "for images, targets, image_ids in model.valid_data_loader:\n",
    "\n",
    "    images = list(image.to(model.device) for image in images)\n",
    "    outputs = model.model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "\n",
    "        \n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################################################################################################\n",
    "# #################### WARNING THIS WILL SHUT DOWN THE COMPUTER IF IT IS UNCOMMENTED ################################\n",
    "# ###################################################################################################################\n",
    "# if ALLOW_AUTOMATIC_SHUTDOWN:\n",
    "#     os.system('shutdown -s -t 0')\n",
    "# ###################################################################################################################\n",
    "# #################### WARNING THIS WILL SHUT DOWN THE COMPUTER IF IT IS UNCOMMENTED ################################\n",
    "# ###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
